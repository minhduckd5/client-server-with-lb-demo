# MODIFY: set these variables in GitLab CI project settings (SSH key, Vagrant config)
# GitLab CI pipeline: vagrant up + ansible configure (K3s) + deploy + smoke test

stages:
  - provision
  - configure
  - deploy
  - smoke-test

variables:
  VAGRANT_WORKING_DIR: infra/vagrant
  # TF_WORKING_DIR: infra/terraform
  ANSIBLE_WORKING_DIR: infra/ansible
  ANSIBLE_INVENTORY: infra/ansible/inventory.ini
  CONTROL_PLANE_IP: "192.168.1.200"
  SSH_USER: "ubuntu"
# PowerShell Error Handling: Stop pipeline immediately if a command fails
  ErrorActionPreference: "Stop"

# ---------------------------------------------------------------------------
# STAGE 1: PROVISION (Native Windows Execution)
# ---------------------------------------------------------------------------

# Provision VMs with Vagrant (manual approval required)
provision:
  stage: provision
  tags:
    - local-dev-pc
  script:
    - Set-Location $env:VAGRANT_WORKING_DIR
    - Write-Host "Attempting to provision with Vagrant..."
    - 'try { vagrant up --provider=vmware_desktop } catch { Write-Warning "Vagrant provision reported error: $_" }'
    # Generate Inventory (Static IPs based on Vagrantfile)
    - Write-Host "Initiating Local Infrastructure Provisioning..."
    
    # 2. Generate Ansible Inventory (PowerShell Syntax)
    - Write-Host "Generating Dynamic Inventory..."
    - $InventoryFile = "$env:CI_PROJECT_DIR\infra\ansible\inventory.ini"
    - |
      $Content = @"
      [k3s_control]
      k3s-control-plane ansible_host=$env:CONTROL_PLANE_IP

      [k3s_workers]
      k3s-worker-1 ansible_host=192.168.1.201
      k3s-worker-2 ansible_host=192.168.1.202

      [k3s_nodes:children]
      k3s_control
      k3s_workers

      [all:vars]
      ansible_user=$env:SSH_USER
      ansible_ssh_private_key_file=/root/.ssh/id_rsa
      ansible_ssh_common_args='-o StrictHostKeyChecking=no'
      "@
      Set-Content -Path $InventoryFile -Value $Content
      
    - Get-Content $InventoryFile
  artifacts:
    paths:
      - infra/ansible/inventory.ini
  only:
    - main
    - develop

# ---------------------------------------------------------------------------
# STAGE 2: CONFIGURE (Docker-Wrapped Ansible)
# ---------------------------------------------------------------------------
# Ansible configuration (K3s)
configure:
  stage: configure
  tags:
    - local-dev-pc
  dependencies:
    - provision
  script:
    - Write-Host "Preparing Transient Ansible Environment..."
        
    # 1. Stage the SSH Key for Docker mounting
    # We copy the GitLab File Variable to a local file so Docker can mount it
    - Get-Content $env:SSH_PRIVATE_KEY | Set-Content -Encoding ascii -Path "id_rsa_temp"

    # 2. Execute Ansible inside a Linux Container
    # We mount the current directory (${PWD}) to /work inside the container
    - |
          docker run --rm `
            -v "${PWD}:/work" `
            -v "${PWD}\id_rsa_temp:/tmp/ssh_key" `
            -w /work/infra/ansible `
            python:3-alpine `
            sh -c "apk add --no-cache openssh-client build-base libffi-dev openssl-dev && \
                   mkdir -p /root/.ssh && \
                   cp /tmp/ssh_key /root/.ssh/id_rsa && \
                   chmod 600 /root/.ssh/id_rsa && \
                   eval \$(ssh-agent -s) && \
                   ssh-add /root/.ssh/id_rsa && \
                   pip install ansible && \
                   mkdir -p /etc/ansible && echo '[defaults]' > /etc/ansible/ansible.cfg && echo 'host_key_checking = False' >> /etc/ansible/ansible.cfg && \
                   ansible-playbook -i inventory.ini bootstrap.yml && \
                   ansible-playbook -i inventory.ini k3s-install.yml && \
                   ansible-playbook -i inventory.ini metallb.yml --extra-vars 'metallb_ip_pool_start=$env:METALLB_IP_POOL_START metallb_ip_pool_end=$env:METALLB_IP_POOL_END' && \
                   ansible-playbook -i inventory.ini build-images.yml && \
                   ansible-playbook -i inventory.ini deploy-nginx-reverse-proxy.yml && \
                   ansible-playbook -i inventory.ini ingress.yml && \
                   ansible-playbook -i inventory.ini monitoring.yml"
            
        # Note: Replace 'site.yml' above with your specific playbooks if you run them individually
        # e.g., "ansible-playbook -i inventory.ini bootstrap.yml && ansible-playbook -i inventory.ini k3s-install.yml ..."

        # 3. Cleanup
    - Remove-Item "id_rsa_temp" -ErrorAction SilentlyContinue
  only:
    - main
    - develop

# Deploy application
# ---------------------------------------------------------------------------
# STAGE 3: DEPLOY (Native Windows SCP + Docker Kubectl)
# ---------------------------------------------------------------------------
deploy:
  stage: deploy
  tags:
    - local-dev-pc
  script:
    - Write-Host "Retrieving Kubeconfig from Control Plane..."
    
    # 1. Fetch kubeconfig using Windows OpenSSH
    # We use the raw SSH key file provided by GitLab
    - scp -o StrictHostKeyChecking=no -i $env:SSH_PRIVATE_KEY ubuntu@${env:CONTROL_PLANE_IP}:~/.kube/config ./kubeconfig_raw
    
    # 2. Adjust Kubeconfig server IP (It might say 127.0.0.1 inside the VM)
    - $Config = Get-Content ./kubeconfig_raw
    - $Config = $Config -replace '127.0.0.1', $env:CONTROL_PLANE_IP
    - $Config | Set-Content ./kubeconfig_final

    # 3. Apply Manifests via Dockerized Kubectl
    - |
      docker run --rm `
        -v "${PWD}:/work" `
        -v "${PWD}\kubeconfig_final:/root/.kube/config" `
        -w /work `
        bitnami/kubectl:latest `
        apply -f k8s/
  dependencies:
    - configure
  only:
    - main
    - develop

# Smoke test
# ---------------------------------------------------------------------------
# STAGE 4: SMOKE TEST (Native PowerShell)
# ---------------------------------------------------------------------------
smoke-test:
  stage: smoke-test
  tags:
    - local-dev-pc
  script:
    - Write-Host "Executing Smoke Tests..."
    
    # Simple HTTP Check
    # Assuming your LoadBalancer exposes a service on a known IP or you query it via kubectl first
    # For this example, we'll check the Nginx endpoint if you know the IP, or just verify the cluster nodes
    
    - docker run --rm -v "${PWD}\kubeconfig_final:/root/.kube/config" bitnami/kubectl:latest get nodes
    
    - Write-Host "Smoke Test Complete."
  dependencies:
    - deploy
  only:
    - main
    - develop