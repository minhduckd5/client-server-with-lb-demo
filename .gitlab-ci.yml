# MODIFY: set these variables in GitLab CI project settings (SSH key, Vagrant config)
# GitLab CI pipeline: vagrant up + ansible configure (K3s) + deploy + smoke test

stages:
  - provision
  - configure
  - deploy
  - smoke-test

variables:
  VAGRANT_WORKING_DIR: infra/vagrant
  # TF_WORKING_DIR: infra/terraform
  ANSIBLE_WORKING_DIR: infra/ansible
  ANSIBLE_INVENTORY: infra/ansible/inventory.ini
  CONTROL_PLANE_IP: "192.168.1.200"
  SSH_USER: "vagrant"
  METALLB_IP_POOL_START: "192.168.1.240"
  METALLB_IP_POOL_END: "192.168.1.250"
# PowerShell Error Handling: Stop pipeline immediately if a command fails
  ErrorActionPreference: "Stop"
  # Prevent git clean from deleting running VM files
  GIT_CLEAN_FLAGS: -ffdx -e infra/vagrant/.vagrant/

# ---------------------------------------------------------------------------
# STAGE 1: PROVISION (Native Windows Execution)
# ---------------------------------------------------------------------------

# Provision VMs with Vagrant (manual approval required)
provision:
  stage: provision
  tags:
    - local-dev-pc
  script:
    - Set-Location $env:VAGRANT_WORKING_DIR
    - Write-Host "Attempting to provision with Vagrant..."
    # Install required plugin if missing
    - 'try { vagrant plugin install vagrant-vmware-desktop } catch { Write-Warning "Plugin install check failed (ignoring if already installed): $_" }'
    - 'try { vagrant up --provider=vmware_desktop } catch { Write-Warning "Vagrant provision reported error: $_" }'
    # Generate Inventory (Static IPs based on Vagrantfile)
    - Write-Host "Initiating Local Infrastructure Provisioning..."
    
    # 2. Generate Ansible Inventory (PowerShell Syntax)
    - Write-Host "Generating Dynamic Inventory..."
    - $InventoryFile = "$env:CI_PROJECT_DIR\infra\ansible\inventory.ini"
    - |
      $Content = @"
      [k3s_control]
      k3s-control-plane ansible_host=$env:CONTROL_PLANE_IP

      [k3s_workers]
      k3s-worker-1 ansible_host=192.168.1.201
      k3s-worker-2 ansible_host=192.168.1.202

      [k3s_nodes:children]
      k3s_control
      k3s_workers

      [all:vars]
      ansible_user=$env:SSH_USER
      ansible_ssh_private_key_file=~/.ssh/insecure_private_key
      ansible_ssh_common_args='-o StrictHostKeyChecking=no'
      "@
      Set-Content -Path $InventoryFile -Value $Content
      
    - Get-Content $InventoryFile
  artifacts:
    paths:
      - infra/ansible/inventory.ini
  only:
    - main
    - develop

# ---------------------------------------------------------------------------
# STAGE 2: CONFIGURE (Docker-Wrapped Ansible)
# ---------------------------------------------------------------------------
# Ansible configuration (K3s)
configure:
  stage: configure
  tags:
    - local-dev-pc
  dependencies:
    - provision
  script:
    - Write-Host "Preparing Transient Ansible Environment..."
        
    # 1. Stage the SSH Key for Docker mounting
    # Try machine-specific key first (Vagrant often rotates keys), then fallback to insecure key
    - $MachineKey = "$env:VAGRANT_WORKING_DIR\.vagrant\machines\k3s-control-plane\vmware_desktop\private_key"
    - $InsecureKey = "$env:USERPROFILE\.vagrant.d\insecure_private_key"
    
    - 'if (Test-Path $MachineKey) { Write-Host "Using machine-specific private key: $MachineKey"; Copy-Item $MachineKey -Destination "id_rsa_temp" } elseif (Test-Path $InsecureKey) { Write-Host "Using global insecure private key: $InsecureKey"; Copy-Item $InsecureKey -Destination "id_rsa_temp" } else { Write-Error "No valid SSH private key found for Vagrant VMs."; exit 1 }'

    # 2. Execute Ansible inside a Linux Container
    # We mount the current directory (${PWD}) to /work inside the container
    # - |
    #       docker run --rm `
    #         -v "${PWD}:/work" `
    #         -v "${PWD}\id_rsa_temp:/tmp/ssh_key" `
    #         -w /work/infra/ansible `
    #         python:3-alpine `
    #         sh -c "apk add --no-cache openssh-client build-base libffi-dev openssl-dev && \
    #                mkdir -p /root/.ssh && \
    #                cp /tmp/ssh_key /root/.ssh/id_rsa && \
    #                chmod 600 /root/.ssh/id_rsa && \
    #                eval \$(ssh-agent -s) && \
    #                ssh-add /root/.ssh/id_rsa && \
    #                pip install ansible && \
    #                mkdir -p /etc/ansible && echo '[defaults]' > /etc/ansible/ansible.cfg && echo 'host_key_checking = False' >> /etc/ansible/ansible.cfg && \
    #                ansible-playbook -i inventory.ini bootstrap.yml && \
    #                ansible-playbook -i inventory.ini k3s-install.yml && \
    #                ansible-playbook -i inventory.ini metallb.yml --extra-vars 'metallb_ip_pool_start=$env:METALLB_IP_POOL_START metallb_ip_pool_end=$env:METALLB_IP_POOL_END' && \
    #                ansible-playbook -i inventory.ini build-images.yml && \
    #                ansible-playbook -i inventory.ini deploy-nginx-reverse-proxy.yml && \
    #                ansible-playbook -i inventory.ini ingress.yml && \
    #                ansible-playbook -i inventory.ini monitoring.yml"
    - |
      $ScriptContent = @"
      # --- START MODIFICATION ---
      # Strong debugging + safety for Ansible pipeline
      set -euo pipefail
      
      echo "=== DEBUG: Environment inside container ==="
      echo "PWD: \$(pwd)"
      echo "Listing /work:"
      ls -R /work || echo "ls /work failed"
      
      echo "=== DEBUG: Installing base tooling ==="
      apk add --no-cache openssh-client build-base libffi-dev openssl-dev
      mkdir -p /root/.ssh
      cp /tmp/ssh_key /root/.ssh/insecure_private_key
      chmod 600 /root/.ssh/insecure_private_key
      
      echo "--- DEBUG: SSH Key Fingerprint ---"
      ssh-keygen -lf /root/.ssh/insecure_private_key || echo "ssh-keygen failed"
      
      echo "--- DEBUG: Ansible Inventory (infra/ansible/inventory.ini) ---"
      cat /work/infra/ansible/inventory.ini || echo "inventory not found at expected path"
      
      echo "--- DEBUG: Testing SSH Connection to Control Plane ---"
      # We explicitly use the user from env var to catch mismatches
      ssh -o StrictHostKeyChecking=no -i /root/.ssh/insecure_private_key -vvv $env:SSH_USER@$env:CONTROL_PLANE_IP 'id' || echo "Manual SSH Test Failed"
      
      eval \$(ssh-agent -s)
      ssh-add /root/.ssh/insecure_private_key || echo "ssh-add failed"
      
      echo "=== DEBUG: Installing Ansible ==="
      pip install ansible
      mkdir -p /etc/ansible
      echo '[defaults]' > /etc/ansible/ansible.cfg
      echo 'host_key_checking = False' >> /etc/ansible/ansible.cfg
      
      run_play() {
        # MODIFIED: escape \$ so PowerShell does not eat shell positional params
        name="`$1"
        shift
        echo "=== DEBUG: START playbook: `$name ==="
        echo "Command: ansible-playbook `$@"
        if ! ansible-playbook "`$@"; then
          echo "=== DEBUG: PLAYBOOK FAILED: `$name ==="
          exit 1
        fi
        echo "=== DEBUG: END playbook: `$name ==="
      }
      
      # MODIFIED: use absolute playbook paths to avoid any working-directory issues
      run_play "bootstrap" -i /work/infra/ansible/inventory.ini /work/infra/ansible/bootstrap.yml
      run_play "k3s-install" -i /work/infra/ansible/inventory.ini /work/infra/ansible/k3s-install.yml
      run_play "metallb" -i /work/infra/ansible/inventory.ini /work/infra/ansible/metallb.yml --extra-vars "metallb_ip_pool_start=$env:METALLB_IP_POOL_START metallb_ip_pool_end=$env:METALLB_IP_POOL_END"
      run_play "build-images" -i /work/infra/ansible/inventory.ini /work/infra/ansible/build-images.yml
      run_play "deploy-nginx-reverse-proxy" -i /work/infra/ansible/inventory.ini /work/infra/ansible/deploy-nginx-reverse-proxy.yml
      run_play "ingress" -i /work/infra/ansible/inventory.ini /work/infra/ansible/ingress.yml
      echo "=== DEBUG: ALL PLAYBOOKS COMPLETED SUCCESSFULLY ==="
      # --- END MODIFICATION ---
      "@
      Set-Content -Path "run_ansible.sh" -Value $ScriptContent -Encoding ascii
    
    - |
      docker run --rm `
        -v "${PWD}:/work" `
        -v "${PWD}\id_rsa_temp:/tmp/ssh_key" `
        -v "${PWD}\run_ansible.sh:/work/run_ansible.sh" `
        -w /work/infra/ansible `
        python:3-alpine `
        sh /work/run_ansible.sh
            
        # Note: Replace 'site.yml' above with your specific playbooks if you run them individually
        # e.g., "ansible-playbook -i inventory.ini bootstrap.yml && ansible-playbook -i inventory.ini k3s-install.yml ..."

        # 3. Cleanup
    - Remove-Item "id_rsa_temp" -ErrorAction SilentlyContinue
    - Remove-Item "run_ansible.sh" -ErrorAction SilentlyContinue
  only:
    - main
    - develop

# Deploy application
# ---------------------------------------------------------------------------
# STAGE 3: DEPLOY (Native Windows SCP + Docker Kubectl)
# ---------------------------------------------------------------------------
deploy:
  stage: deploy
  tags:
    - local-dev-pc
  script:
    - Write-Host "Retrieving Kubeconfig from Control Plane..."
    
    # 1. Fetch kubeconfig using Windows OpenSSH
    # We use the raw SSH key file provided by GitLab
    - scp -o StrictHostKeyChecking=no -i $env:SSH_PRIVATE_KEY ubuntu@${env:CONTROL_PLANE_IP}:~/.kube/config ./kubeconfig_raw
    
    # 2. Adjust Kubeconfig server IP (It might say 127.0.0.1 inside the VM)
    - $Config = Get-Content ./kubeconfig_raw
    - $Config = $Config -replace '127.0.0.1', $env:CONTROL_PLANE_IP
    - $Config | Set-Content ./kubeconfig_final

    # 3. Apply Manifests via Dockerized Kubectl
    - |
      docker run --rm `
        -v "${PWD}:/work" `
        -v "${PWD}\kubeconfig_final:/root/.kube/config" `
        -w /work `
        bitnami/kubectl:latest `
        apply -f k8s/
  dependencies:
    - configure
  only:
    - main
    - develop

# Smoke test
# ---------------------------------------------------------------------------
# STAGE 4: SMOKE TEST (Native PowerShell)
# ---------------------------------------------------------------------------
smoke-test:
  stage: smoke-test
  tags:
    - local-dev-pc
  script:
    - Write-Host "Executing Smoke Tests..."
    
    # Simple HTTP Check
    # Assuming your LoadBalancer exposes a service on a known IP or you query it via kubectl first
    # For this example, we'll check the Nginx endpoint if you know the IP, or just verify the cluster nodes
    
    - docker run --rm -v "${PWD}\kubeconfig_final:/root/.kube/config" bitnami/kubectl:latest get nodes
    
    - Write-Host "Smoke Test Complete."
  dependencies:
    - deploy
  only:
    - main
    - develop